import json
import boto3
import psycopg2
import csv
from io import StringIO
import os

s3_client = boto3.client('s3')

def lambda_handler(event, context):
    # Database connection parameters
    conn = psycopg2.connect(
        host=os.environ['DB_HOST'],
        database=os.environ['DB_NAME'],
        user=os.environ['DB_USER'],
        password=os.environ['DB_PASSWORD'],
        port=int(os.environ.get('DB_PORT', '5432'))
    )
    
    cursor = conn.cursor()
    output = StringIO()
    writer = csv.writer(output)
    
    # Get all tables
    cursor.execute("""
        SELECT table_name 
        FROM information_schema.tables 
        WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
    """)
    tables = [row[0] for row in cursor.fetchall()]
    
    # Export each table
    for table in tables:
        # Write table header
        writer.writerow([f"TABLE: {table}"])
        
        # Get column names
        cursor.execute(f"SELECT * FROM {table} LIMIT 0")
        columns = [desc[0] for desc in cursor.description]
        writer.writerow(columns)
        
        # Get all data
        cursor.execute(f"SELECT * FROM {table}")
        writer.writerows(cursor.fetchall())
        writer.writerow([])  # Empty row between tables
    
    # Upload to S3
    s3_client.put_object(
        Bucket=os.environ['S3_BUCKET'],
        Key=f"db-export-{context.request_id}.csv",
        Body=output.getvalue()
    )
    
    cursor.close()
    conn.close()
    
    return {
        'statusCode': 200,
        'body': json.dumps(f"Export completed: db-export-{context.request_id}.csv")
    }
